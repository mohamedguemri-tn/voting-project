# **Complete Lab (part2) â€“ CI/CD Deployment of the Voting App to AWS EKS using GitHub Actions**

---

## **Objectives**


At the end of this lab, you will:

* Build Docker images for all **Voting App** microservices.
* Push them automatically to **Amazon Elastic Container Registry (ECR)**.
* Deploy and update the application on **Amazon Elastic Kubernetes Service (EKS)** using **Helm**.
* Orchestrate the entire workflow through **GitHub Actions**.

---

## **1 â€“ Overview**

You previously packaged the app into a Helm chart and deployed it locally.
Now, you will move that process into the cloud, with AWS as the runtime and GitHub Actions as the automation engine.

| Step | Tool               | Purpose                   |
| ---- | ------------------ | ------------------------- |
| 1    | **GitHub Actions** | CI/CD orchestration       |
| 2    | **ECR**            | Image registry            |
| 3    | **EKS**            | Kubernetes hosting        |
| 4    | **Helm**           | Application packaging     |
| 5    | **IAM**            | Secure AWS authentication |

---

## **2 â€“ Prerequisites**

| Requirement        | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| AWS Account        | With rights to create IAM users, ECR repos, and EKS clusters |
| EKS Cluster        | Running (example : `voting-cluster`)                         |
| Helm Chart         | From Lab 21 (`voting-app/`)                                  |
| GitHub Repository  | Containing your app source and chart                         |
| Docker Installed   | To test builds locally                                       |
| Kubectl Configured | Access to your EKS cluster verified                          |

---

# **2.1 â€“ Create an Amazon EKS Cluster (Auto Mode) via Console**

---

## **1. Open the EKS Console**

* Go to [https://console.aws.amazon.com/eks](https://console.aws.amazon.com/eks)
* Click **â€œAdd clusterâ€ â†’ â€œCreateâ€**

Youâ€™ll arrive on the **Cluster configuration** page.

---

## **2. Choose Configuration Mode**

When prompted:

> **Cluster configuration options**

Select â†’ **Quick configuration (with EKS Auto Mode)**
*(recommended for labs and production-ready defaults)*

This will automatically manage networking, nodes, and storage.

---

## **3. Cluster Configuration**

| Field                  | Action                                        |
| ---------------------- | --------------------------------------------- |
| **Name**               | Enter a unique name, e.g. `voting-cluster`    |
| **Kubernetes version** | Leave default (latest version)                |
| **Cluster IAM role**   | You must create or select one (see next step) |

---

## **4. Create the Cluster IAM Role**

This role allows the EKS **control plane** to manage AWS resources such as networking, storage, and load balancers.

### **Step 1 â€“ Go to IAM â†’ Roles â†’ Create role**

* **Trusted entity type:** AWS Service
* **Use case:** search and select `EKS` â†’ choose **EKS - Cluster**
* Click **Next**

### **Step 2 â€“ Attach Policies**

Select and attach **all five managed policies**:

```
AmazonEKSClusterPolicy
AmazonEKSBlockStoragePolicy
AmazonEKSComputePolicy
AmazonEKSLoadBalancingPolicy
AmazonEKSNetworkingPolicy
```
![img_7.png](images/img_7.png)

Click **Next**, name the role:

```
AmazonEKSClusterRole
```

Then click **Create role**.

---

### **Step 3 â€“ Edit Trust Relationship**

Once created:

1. Open the role â†’ **Trust relationships â†’ Edit trust policy**
2. Replace its JSON with:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "eks.amazonaws.com"
      },
      "Action": [
        "sts:AssumeRole",
        "sts:TagSession"
      ]
    }
  ]
}
```

![img_8.png](images/img_8.png)

3. Save changes.

---

**Result:**
You now have a cluster role named `AmazonEKSClusterRole` with the correct trust policy and permissions.

Return to the EKS console â†’ **Refresh**, and select this role under **Cluster IAM role**.

---

## **5. Create the Node IAM Role**

Worker nodes (EC2 or managed nodes) need their own IAM role so they can join the cluster and pull images.

### **Step 1 â€“ Go to IAM â†’ Roles â†’ Create role**

* **Trusted entity type:** AWS Service
* **Use case:** search for `EKS` â†’ choose **EKS - Node**
* Click **Next**

### **Step 2 â€“ Attach Policies**

Attach these **three managed policies**:

```
AmazonEKSWorkerNodePolicy
AmazonEC2ContainerRegistryReadOnly

```

Click **Next**, then name it:

```
AmazonEKSNodeRole
```

Click **Create role**.

---

### **Step 3 â€“ Verify Trust Relationship**

It should look like this by default:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

If it matches, leave it as is.

---

**Result:**
Your **Node Role** is now ready â€” `AmazonEKSNodeRole`.

---

## **6. Continue Cluster Setup**

Back in the **EKS Console â†’ Create Cluster** wizard:

| Section              | Action                                                                 |
| -------------------- | ---------------------------------------------------------------------- |
| **Cluster IAM role** | Select `AmazonEKSClusterRole`                                          |
| **Node IAM role**    | Select `AmazonEKSNodeRole`                                             |
| **VPC**              | Choose an existing VPC (or let EKS Auto Mode create one)               |
| **Subnets**          | Select at least two public subnets (or let EKS Auto Mode create one)                                       |

![img_9.png](images/img_9.png)

Then click **Next â†’ Create cluster**.

---

## **7. Wait for Cluster Creation**

EKS will provision the control plane and networking.
This can take **10â€“15 minutes**.

Once done, the cluster status becomes:

```
Active
```

![img_10.png](images/img_10.png)

---

## **8. Verify the Cluster**

### via Console

Now connect using:

```bash
aws eks update-kubeconfig --name voting-cluster
kubectl get nodes
```

![img_11.png](images/img_11.png)

---

You now have a **fully functional EKS Auto Mode cluster**, ready for your **GitHub Actions CI/CD deployment **.




## **3 â€“ Create an IAM User for GitHub Actions**

GitHub Actions needs its own AWS credentials to push images and deploy updates.

1. Go to **AWS Console â†’ IAM â†’ Users â†’ Add user**.

2. **User name:** `github-actions`.

3. Enable **Programmatic access** (no console access).

4. Click **Next: Permissions â†’ Attach existing policies directly**.

5. Select these managed policies:

   ```
   AmazonEC2ContainerRegistryFullAccess
   AmazonEKSClusterPolicy
   AmazonEKSWorkerNodePolicy
   AmazonEKSServicePolicy
   ```

6. Finish â†’ **Create user**.

7. Copy the **Access key ID** and **Secret access key** â€” they will become GitHub secrets.

    * **Access key ID = AWS_ACCESS_KEY_ID**
    * **Secret access key = AWS_SECRET_ACCESS_KEY**
---

ðŸ’¡ **Hint:** This user does **not** need console access, only API access.

## 3.1 â€“ Granting GitHub Actions Access to EKS

When a GitHub Actions workflow deploys to an EKS cluster using Helm, it authenticates with an **IAM user** (for example, `github-actions`) defined by the secrets
`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.

By default, this IAM user **does not have Kubernetes-level (RBAC) permissions**, so it cannot list or create Kubernetes resources such as `secrets`, `pods`, or `deployments`.
To fix this, we must explicitly grant the IAM user **cluster-admin** privileges inside the EKS cluster.

---

### Step 1 â€“ Connect to the cluster using AWS CloudShell

1. Open **AWS CloudShell** from the AWS Console (top right corner).
2. Connect to your EKS cluster by running:

   ```bash
   aws eks update-kubeconfig --name extravagant-bluegrass-orca --region eu-north-1
   ```

   This command configures `kubectl` to communicate with your EKS cluster.

---

### Step 2 â€“ Create a ClusterRoleBinding for the GitHub IAM user

Run the following command directly in CloudShell:

```bash
kubectl apply -f - <<'EOF'
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: github-actions-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: User
    name: arn:aws:iam::618180422329:user/github-actions
    apiGroup: rbac.authorization.k8s.io
EOF
```

This creates a **ClusterRoleBinding** named `github-actions-admin` that links the IAM user `github-actions` to the `cluster-admin` role.
It gives the GitHub Actions workflow full access to manage all Kubernetes resources in the cluster.

---

### Step 3 â€“ Register the IAM user in the EKS Access UI

In the AWS Console:

1. Go to **EKS â†’ Clusters â†’ voting-app â†’ Access**.
   ![g2.png](images/g2.png)
2. Click **Add access entry**.
3. For **Principal type**, choose **IAM user**, then select your `github-actions` user.
   ![g3.png](images/g3.png)
4. Do **not** add any access policies â€” this step is optional.
   ![g4.png](images/g4.png)
5. Click **Create**.
   ![g5.png](images/g5.png)
   This UI action registers the IAM user with the EKS clusterâ€™s access control system.
   ![g6.png](images/g6.png)
   Combined with the previous `ClusterRoleBinding`, it ensures that GitHub Actions can both **authenticate** (via IAM) and **authorize** (via RBAC) to manage Kubernetes resources.

---

By combining both steps:

* **EKS Access Entry** â†’ allows the IAM user to connect to the cluster.
* **ClusterRoleBinding** â†’ gives that user Kubernetes admin privileges.

Together, they enable your GitHub Actions pipeline to perform Helm deployments and manage resources seamlessly within EKS.


## **4 â€“ Create or Identify Your ECR Registry**

ECR stores Docker images. Each service (vote, result, etc.) will have its own repository.

### What is it?

Your ECR registry URL identifies your AWS account and region.
It looks like this:

```
<account-id>.dkr.ecr.<region>.amazonaws.com
```

### **Find Your ECR Registry URL via Console**

1. Go to **ECR â†’ Repositories**
2. Click **Create ** (top right)

![img_5.png](images/img_5.png)

3. Copy the line that looks like this:

   ðŸ‘‰ **`618180422329.dkr.ecr.eu-west-3.amazonaws.com`**

Use that for the GitHub secret `ECR_REGISTRY`.

---

## **5 â€“ Add AWS Secrets to GitHub**

In your GitHub repository:
**Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret**

Create these entries:

| Secret Name             | Example Value                                |
| ----------------------- | -------------------------------------------- |
| `AWS_ACCESS_KEY_ID`     | AKIAâ€¦                                        |
| `AWS_SECRET_ACCESS_KEY` | wJalrâ€¦                                       |
| `AWS_REGION`            | eu-west-3                                    |
| `ECR_REGISTRY`          | 618180422329.dkr.ecr.eu-west-3.amazonaws.com |
| `EKS_CLUSTER_NAME`      | voting-cluster                               |
| `K8S_NAMESPACE`         | voting-app                                   |

All six should appear in your secrets list.

![img_6.png](images/img_6.png)

---

## **6 â€“ Prepare the Project Structure**

```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ eks-cicd.yml
â”œâ”€â”€ voting-app/
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ vote/
â”‚       â”œâ”€â”€ result/
â”‚       â”œâ”€â”€ worker/
â”‚       â”œâ”€â”€ redis/
â”‚       â””â”€â”€ db/
â”œâ”€â”€ vote/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ vote-ui/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ result/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ result-ui/
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ worker/
    â””â”€â”€ Dockerfile
```

Each service folder contains its Dockerfile from previous labs.

---

## **7 â€“ Create the GitHub Actions Workflow Step by Step**

Now weâ€™ll automate the deployment of the **Voting App** to **AWS EKS** using **GitHub Actions**.
The pipeline has **two jobs**:

1. **Build and Push** â†’ build Docker images and push them to **ECR**
2. **Deploy to EKS** â†’ deploy and upgrade the Helm chart on your EKS cluster

Everything goes inside:
`.github/workflows/eks-cicd.yml`

---

### **7.1 â€“ Define When the Workflow Runs**

We want to:

* run automatically whenever someone pushes to the `main` branch, and
* also trigger it manually from GitHubâ€™s **Actions** tab.

```yaml
on:
  push:
    branches: [ main ]
  workflow_dispatch:
```

ðŸ’¡ **Hint:**
`workflow_dispatch` lets you run the pipeline manually â€” useful for testing.

---

### **7.2 â€“ Declare Global Environment Variables**

Weâ€™ll reuse these values multiple times across both jobs.

```yaml
env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
  K8S_NAMESPACE: ${{ secrets.K8S_NAMESPACE }}
  IMAGE_TAG: ${{ github.sha }}
```

ðŸ’¡ **Hint:**
`github.sha` automatically sets the Docker image tag to the current commit hash â€” a simple versioning mechanism.

---

### **7.3 â€“ Job 1: Build and Push Images**

Name: `build-and-push`
Goal: build the Docker images and push them to your **Amazon ECR** registry.

---

#### Step 1 â€“ Checkout the Code

```yaml
- name: Checkout code
  uses: actions/checkout@v4
```

ðŸ’¡ This makes the source code available to the GitHub Actions runner.

---

#### Step 2 â€“ Configure AWS Credentials

```yaml
- name: Configure AWS credentials
  uses: aws-actions/configure-aws-credentials@v4
  with:
    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    aws-region: ${{ secrets.AWS_REGION }}
```

ðŸ’¡ **Hint:**
This authenticates your workflow to AWS using the secrets you added earlier.

---

#### Step 3 â€“ Log in to Amazon ECR

```yaml
- name: Log in to Amazon ECR
  id: login-ecr
  uses: aws-actions/amazon-ecr-login@v2
```

ðŸ’¡ This automatically runs `docker login` with your ECR credentials.

---

#### Step 4 â€“ Make Sure ECR Repositories Exist

Before pushing, we check if each serviceâ€™s ECR repository exists â€” if not, create it.

```yaml
- name: Ensure ECR repositories exist
  run: |
    SERVICES=("vote" "vote-ui" "result" "result-ui" "worker")
    for SERVICE in "${SERVICES[@]}"; do
      echo "Ensuring repo $SERVICE"
      aws ecr describe-repositories --repository-names $SERVICE >/dev/null 2>&1 || \
      aws ecr create-repository --repository-name $SERVICE --region $AWS_REGION
    done
```

ðŸ’¡ **Hint:**
This makes your workflow **idempotent** â€” you can rerun it without errors even if repos already exist.

---

#### Step 5 â€“ Build and Push Docker Images

```yaml
- name: Build and Push Docker images
  env:
    ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
  run: |
    SERVICES=("vote" "vote-ui" "result" "result-ui" "worker")
    for SERVICE in "${SERVICES[@]}"; do
      IMAGE_URI="$ECR_REGISTRY/$SERVICE:${IMAGE_TAG}"
      echo "Building $SERVICE â†’ $IMAGE_URI"
      if [ "$SERVICE" = "worker" ]; then
        docker build -t "$IMAGE_URI" -f "./$SERVICE/src/Dockerfile" "./$SERVICE/src"
      else
        docker build -t "$IMAGE_URI" "./$SERVICE"
      fi
      docker push "$IMAGE_URI"
    done
```

ðŸ’¡ **Hint:**
This loops over all five services, builds each image, tags it with the current commit hash, and pushes it to your ECR registry.

---

### **7.4 â€“ Job 2: Deploy to EKS**

Name: `deploy-to-eks`
Goal: deploy the latest Docker images using the Helm chart.

We make this job **depend on** the previous one:

```yaml
needs: build-and-push
```

---

#### Step 1 â€“ Checkout the Code Again

```yaml
- name: Checkout code
  uses: actions/checkout@v4
```

---

#### Step 2 â€“ Configure AWS Credentials Again

```yaml
- name: Configure AWS credentials
  uses: aws-actions/configure-aws-credentials@v4
  with:
    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    aws-region: ${{ secrets.AWS_REGION }}
```

---

#### Step 3 â€“ Connect to the EKS Cluster

```yaml
- name: Connect to EKS cluster
  run: aws eks update-kubeconfig --name ${{ secrets.EKS_CLUSTER_NAME }} --region ${{ secrets.AWS_REGION }}
```

ðŸ’¡ This sets up `kubectl` so the workflow can talk to your cluster.

---

#### Step 4 â€“ Deploy the Helm Chart

```yaml
- name: Deploy Helm Chart
  env:
    ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  run: |
    helm upgrade --install voting ./voting-app \
      --namespace ${{ secrets.K8S_NAMESPACE }} \
      --create-namespace \
      --set vote.image.repository=$ECR_REGISTRY/vote \
      --set vote.image.tag=${{ github.sha }} \
      --set voteUi.image.repository=$ECR_REGISTRY/vote-ui \
      --set voteUi.image.tag=${{ github.sha }} \
      --set result.image.repository=$ECR_REGISTRY/result \
      --set result.image.tag=${{ github.sha }} \
      --set resultUi.image.repository=$ECR_REGISTRY/result-ui \
      --set resultUi.image.tag=${{ github.sha }} \
      --set worker.image.repository=$ECR_REGISTRY/worker \
      --set worker.image.tag=${{ github.sha }}
```

By default, the Helm chart exposes **UI services** (`vote-ui`, `result-ui`) as `NodePort` â€” suitable for local clusters (like Minikube).
On AWS EKS, we override this behavior in the CI/CD pipeline to create **public LoadBalancers** automatically.

This is achieved using the following flags in the Helm command:

```bash
--set voteUi.service.type=LoadBalancer \
--set resultUi.service.type=LoadBalancer
```

This tells Kubernetes to create an **internet-facing AWS LoadBalancer** for both UIs when deployed on EKS.

ðŸ’¡ **Hint:**
`helm upgrade --install` ensures that:

* if the release doesnâ€™t exist, itâ€™s created;
* if it does, itâ€™s upgraded with the new image tags.

---

#### Step 5 â€“ Verify the Deployment

```yaml
- name: Verify deployment
  run: |
    kubectl get pods -n ${{ secrets.K8S_NAMESPACE }}
    kubectl get svc -n ${{ secrets.K8S_NAMESPACE }}
```

ðŸ’¡ This simply lists the pods and services â€” you should see the new versions rolling out.

---

### **7.5 â€“ Complete Workflow File**

Once youâ€™ve understood each part, hereâ€™s the full workflow together:

```yaml
name: Voting App â€“ Build & Deploy to AWS EKS (ARM64 + Cache)

on:
   push:
      branches: [ main ]
   workflow_dispatch:

env:
   AWS_REGION: ${{ secrets.AWS_REGION }}
   ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
   EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
   K8S_NAMESPACE: ${{ secrets.K8S_NAMESPACE }}
   IMAGE_TAG: ${{ github.sha }}

jobs:

   build:
      name: Build and Push ARM64 Docker Images
      runs-on: ubuntu-latest
      outputs:
         image_tag: ${{ env.IMAGE_TAG }}

      steps:
         - uses: actions/checkout@v4

         - uses: aws-actions/configure-aws-credentials@v4
           with:
              aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
              aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
              aws-region: ${{ env.AWS_REGION }}

         - uses: docker/setup-buildx-action@v3

         - name: Cache Docker layers
           uses: actions/cache@v4
           with:
              path: /tmp/.buildx-cache
              key: ${{ runner.os }}-buildx-${{ github.ref_name }}
              restore-keys: |
                 ${{ runner.os }}-buildx-

         - name: Login to Amazon ECR
           run: aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REGISTRY

         - name: Ensure ECR repositories exist
           run: |
              for repo in vote vote-ui result result-ui worker; do
                aws ecr describe-repositories --repository-names $repo >/dev/null 2>&1 || \
                aws ecr create-repository --repository-name $repo --region $AWS_REGION
              done

         - name: Build and Push ARM64 Docker Images
           run: |
              for svc in vote vote-ui result result-ui worker; do
                echo "Building $svc â†’ $ECR_REGISTRY/$svc:$IMAGE_TAG"
                if [ "$svc" = "worker" ]; then
                  docker buildx build --platform linux/arm64 \
                    --cache-from type=local,src=/tmp/.buildx-cache \
                    --cache-to type=local,dest=/tmp/.buildx-cache,mode=max \
                    -t $ECR_REGISTRY/$svc:$IMAGE_TAG \
                    -f "./$svc/src/Dockerfile" "./$svc/src" --push
                else
                  docker buildx build --platform linux/arm64 \
                    --cache-from type=local,src=/tmp/.buildx-cache \
                    --cache-to type=local,dest=/tmp/.buildx-cache,mode=max \
                    -t $ECR_REGISTRY/$svc:$IMAGE_TAG "./$svc" --push
                fi
              done

   deploy:
      name: Deploy Voting App to AWS EKS
      runs-on: ubuntu-latest
      needs: build

      steps:
         - uses: actions/checkout@v4

         - uses: aws-actions/configure-aws-credentials@v4
           with:
              aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
              aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
              aws-region: ${{ env.AWS_REGION }}

         - name: Connect to EKS Cluster
           run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

         - name: Deploy via Helm
           run: |
              echo "Using registry: $ECR_REGISTRY"
              helm upgrade --install voting ./voting-app \
                --namespace $K8S_NAMESPACE --create-namespace \
                --set vote.image.repository=$ECR_REGISTRY/vote \
                --set vote.image.tag=$IMAGE_TAG \
                --set voteUi.image.repository=$ECR_REGISTRY/vote-ui \
                --set voteUi.image.tag=$IMAGE_TAG \
                --set result.image.repository=$ECR_REGISTRY/result \
                --set result.image.tag=$IMAGE_TAG \
                --set resultUi.image.repository=$ECR_REGISTRY/result-ui \
                --set resultUi.image.tag=$IMAGE_TAG \
                --set worker.image.repository=$ECR_REGISTRY/worker \
                --set worker.image.tag=$IMAGE_TAG \
                --set voteUi.service.type=LoadBalancer \
                --set resultUi.service.type=LoadBalancer

         - name: Verify Deployment
           run: |
              kubectl get pods -n $K8S_NAMESPACE
              kubectl get svc -n $K8S_NAMESPACE
```

---
### **7.6 â€“ Push the Helm Chart to Amazon ECR (as OCI Artifact)**

In addition to Docker images, you can store your **Helm chart** directly inside Amazon ECR.
Helm v3 supports OCI (Open Container Initiative) format, and ECR acts as a secure OCI registry.

This step allows the chart to be versioned and pulled directly from ECR later â€” useful for multi-cluster deployments or other CI/CD tools.

---

### **Step 1 â€“ Enable Helm OCI Support**

Helm v3 has it built-in, but you must set this environment variable:

```bash
export HELM_EXPERIMENTAL_OCI=1
```

---

### **Step 2 â€“ Authenticate Helm to ECR**

```bash
aws ecr get-login-password --region $AWS_REGION | helm registry login --username AWS --password-stdin $ECR_REGISTRY
```

---

### **Step 3 â€“ Package and Push the Chart**

```bash
helm package ./voting-app
helm push voting-app-*.tgz oci://$ECR_REGISTRY/helm/voting-app
```

* `helm package` creates `voting-app-<version>.tgz` (based on Chart.yaml).
* `helm push` uploads it to your ECR under the repository path `helm/voting-app`.

---

### **Step 4 â€“ Confirm Push**

To verify that your Helm chart is stored:

```bash
helm pull oci://$ECR_REGISTRY/helm/voting-app --version <chart-version>
```

This ensures your chart is retrievable from ECR.

---

### ðŸ’¡ **Hint**

You can later deploy directly from the ECR Helm repository instead of from source:

```bash
helm install voting oci://$ECR_REGISTRY/helm/voting-app --version <chart-version>
```

---

## **Updated YAML Workflow Snippet**

Add this step to your GitHub Actions workflow right after image push (still inside the **build** job):

```yaml
- name: Push Helm chart to Amazon ECR (OCI)
  env:
    ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
    AWS_REGION: ${{ secrets.AWS_REGION }}
  run: |
    export HELM_EXPERIMENTAL_OCI=1
    aws ecr get-login-password --region $AWS_REGION | helm registry login --username AWS --password-stdin $ECR_REGISTRY
    helm package ./voting-app
    helm push $(ls voting-app-*.tgz) oci://$ECR_REGISTRY/helm/voting-app
```

This will:

1. Log in to ECR for Helm.
2. Package your chart.
3. Push it to an OCI repository `helm/voting-app`.

---

### **Bonus â€“ Create the Helm Repository Automatically (optional)**

If it doesnâ€™t exist, ECR will create it automatically during the push, but you can also ensure it beforehand:

```yaml
- name: Ensure Helm ECR repo exists
  run: |
    aws ecr describe-repositories --repository-names helm/voting-app >/dev/null 2>&1 || \
    aws ecr create-repository --repository-name helm/voting-app --region $AWS_REGION
```

Place it just before the `helm push` step.

---

## **Result**

After this addition, your CI/CD pipeline now performs:

| Stage   | Action                           | Output                      |
| ------- | -------------------------------- | --------------------------- |
| Build   | Build and push 5 Docker images   | `ECR/<service>:<sha>`       |
| Package | Package Helm chart               | `voting-app-<ver>.tgz`      |
| Push    | Upload Helm chart to ECR (OCI)   | `ECR/helm/voting-app:<ver>` |
| Deploy  | Install chart from source or ECR | Running pods on EKS         |

---

## **8 â€“ How It Works**

| Step              | Description                                      |
| ----------------- | ------------------------------------------------ |
| Checkout          | Pulls repo content into the runner               |
| Configure AWS     | Authenticates GitHub Actions to AWS              |
| ECR Login         | Logs Docker into your private registry           |
| Ensure Repos      | Creates missing ECR repos automatically          |
| Build + Push      | Builds all Docker images and uploads them to ECR |
| Update Kubeconfig | Connects kubectl to EKS                          |
| Helm Upgrade      | Deploys the Helm chart with the new image tags   |
| Verify            | Lists running pods and services                  |

---

## **9 â€“ Trigger the Pipeline**

Commit and push:

```bash
git add .
git commit -m "Add CI/CD workflow for AWS EKS"
git push origin main
```

Then open **GitHub â†’ Actions**.
Youâ€™ll see two jobs running:

1. **Build and Push Docker images**
2. **Deploy to EKS with Helm**

Both should complete successfully.

---

## **10 â€“ Verify Deployment on EKS**

Check the application state:

```bash
kubectl get pods -n voting-app
kubectl get svc -n voting-app
```

If your `vote-ui` and `result-ui` services are **NodePort** or **LoadBalancer**, display the external address:

```bash
kubectl get svc vote-ui -n voting-app
```

Then open:

```
http://<EXTERNAL-IP>:80
```

You should see the voting interface.

---

## **11 â€“ Cleanup**

Remove all resources:

```bash
helm uninstall voting -n voting-app
kubectl delete ns voting-app
```

Optionally, delete ECR repositories from the AWS Console if you no longer need them.

---

## **12 â€“ Summary**

| Component          | Role                                 |
| ------------------ | ------------------------------------ |
| **Helm Chart**     | Packages Kubernetes manifests        |
| **GitHub Actions** | Automates CI/CD                      |
| **ECR**            | Hosts built Docker images            |
| **EKS**            | Runs the production workloads        |
| **IAM User**       | Provides GitHub â†’ AWS authentication |

Each push to `main` now automatically:

1. Builds and tags the microservice images.
2. Pushes them to ECR.
3. Deploys them to EKS via Helm.
4. Verifies rollout.

---

**End of Lab**

You now own a **complete production-grade CI/CD pipeline**: GitHub â†’ AWS ECR â†’ EKS, fully automated through Helm.
